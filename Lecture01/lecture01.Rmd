---
title: "AMSI Winter </br> School 2021"
subtitle: "Dimension Reduction"
author: "Anastasios Panagiotelis"
institute: "University of Sydney"
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css: [default,"css/mtheme.css","css/mod.css"]
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    includes:
      before_body: aux/defs.html
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(gifski)
library(ggrepel)
library(ggmap)
```

---

class: center, middle, inverse

# Motivation

---

# Finding structure

- Examples
- Simple structure from high-dimensional data

---

class: center, middle, inverse

# Principal Components Analysis

---

# Explaining Variance

- Let there be $n$ observations of $p$ variables; $x_{ij}$ denotes observation $i$ and variable $j$.
--

- Find some linear combination of variables that has maximal variance.
--

- Find $w_1,w_2,\dots,w_p$ such that

$$y_i=w_1x_{i1}+w_2x_{i2}+\dots w_px_{ip}$$
has the biggest possible variance.
--

This is called the first principal component (PC).


---

# More PCs

- After finding the first principal component can look for a linear combination that
--

  + Has maximum variance
  + Is uncorrelated with the first PC
--

- This is called the second principal component
--

- This continues until there are as many PCs as variables.


---


# No cheating...

- Arbitrarily big weights
--
$\rightarrow$ arbitrarily big variance.
--

  + Constrain $\sum w_j=1$
--

- Sensitive to units of measurement.
--

  + Center all variables by subtracting the mean.
  + Standardise all variables to have unit variance.
  
---

# An example

```{r,message=F,echo=F}

library(tidyverse)

wb<-read_csv('data/WorldBank.csv',na = '..',n_max = 43617)

wb%>%
  filter(!(`Country Name`%in%c('India','China')))%>%
  select(`Country Name`,`Country Code`,Series=`Series Code`,Value=`2015 [YR2015]`)%>%
  pivot_wider(id_cols = 1:2,names_from = Series,values_from=Value)->wb_rs

mis<-apply(wb_rs,2,function(x){sum(is.na(x))})

k<-50

wb_sel<-wb_rs[,(mis<k)]

wb_clean<-wb_sel[complete.cases(wb_sel),]

write_csv(wb_clean,'data/WorldBankClean.csv')

```

- Data obtained from the World Bank for 132 countries and 67 variables.
--

- Variables include:
--

  + Health indicators and life expectancy
  + Technology adoption
  + Education rates
  + Economic indicators 
--

- Some variables and observations removed to reduce missing data.

---

# Data

```{r,message=F,echo=F}
library(knitr)
library(kableExtra)
wb<-read_csv('data/WorldBankClean.csv')
kable(wb,format = 'html')%>%
  kable_styling(font_size = 9,bootstrap_options = c("striped","hover","condensed"))%>%
  scroll_box(height="400px")

```

For more on data LINK

---

# Implementation

R Code to implement PCA
--

```{r,message=F,echo=T}
library(tidyverse)
library(broom)
wb<-read_csv('data/WorldBankClean.csv')
wb%>%
  select_if(.,is.numeric)%>% #Use numeric data
  scale()%>% #Standardise
  prcomp()->pca #Compute PCs
wbPC<-augment(pca,wb) #Add PCs to dataframe

```

---

# Scree plot

```{r, echo=F}
tibble(Variance=pca$sdev^2,Component=1:length(pca$sdev))%>%
  ggplot(aes(x=Component,y=Variance))+geom_line()+geom_hline(yintercept=1,col='blue')
```

---

# Plot

```{r,message=F,echo=F,eval=T,message=F,warning=F}
library(plotly)
library(widgetframe)

wbPC%>%
  ggplot(aes(x=.fittedPC1,y=.fittedPC2,label=`Country Code`,text=`Country Name`))+geom_point(size=0.2)+
  geom_text(size=3,nudge_x=0.1)+xlab('Principal Component 1')+ylab('Principal Component 2')+coord_fixed()->g1

ggplotly(g1,dynamicTicks = T,tooltip="text")%>%
  frameWidget(width="100%",height="100%")

```

---

# Uncovering Structure

- Countries towards the right tend to be more economically developed.
--

- Countries towards the bottom tend to be larger in population.
--

- Countries that are similar to one another are closer together on the plot.
--

- A small number of $m<<p$ PCs is sufficient to explain a large proportion of variance.

---
class: middle, center, inverse

# PCA: The Algebra

---

# PCA as optimisation

- LC given by $\by=\bX\bw$
--

- Variance of LC: $\frac{1}{n-1}\sum_{i=1}^n y^2_i=\frac{1}{n-1}\by'\by$
--

- Optimisation problem is
$$\underset{\bw}{argmax}\,\frac{1}{n-1}\bw'\bX'\bX\bw$$

subject to $\bw'\bw=1$
--

- Replace $\bS=\frac{1}{n-1}\bX'\bX$
---

# Solution

- Lagrangian is

$$\calL=\bw'\bS\bw-\lambda(\bw'\bw-1)$$
--

- First order conditions

$$\frac{\partial\calL}{\partial{\bw}}=2\bS\bw-2\lambda\bw$$
--

- Need to find $\mathbf{w}$ to satisfy

$$\bS\bw=\lambda\bw$$
---

# Eigenvalue Decomposition

- Solutions are given by the eigenvalue decomposition.
--

- There are multiple solutions. The eigenvector corresponding to the largest eigenvalue gives the weights of the first principal component.
--

- The eigenvector corresponding the the second largest eigenvalue gives the weights of the second principal component.
--

- And so on...

---

# Data compression

- For the eigendecomposition

$$\bS=\sum_{j=1}^p \lambda_j\bw_j\bw_j'$$
- This can be approximated by

$$\bS\approx\sum_{j=1}^{\color{blue}{m}} \lambda_j\bw_j\bw_j'$$
---

class: inverse, middle, center

# PCA: The geometry

---

# Rotations

- A matrix of eigenvectors $\bW$ is a rotation matrix
--
  
  + Columns/rows are orthogonal
  + Columns/rows have unit length
--

- Multiplying a vector by rotation matrix literally rotates that vector.

---

# Rotation is PCA

- Principal components given by $\bY=\bX\bW$
--

- Each observation (row of $\bX$) is rotated to new components
--

- This is best seen with a simple example

---

# A simple case

```{r,echo=F,fig.height=5.5}


wb%>%
  select(IT.NET.USER.ZS,SH.ANM.NPRG.ZS)%>%
  scale()->wbsimp #Use numeric data
wbsimp%>% #Standardise
  prcomp()->pca #Compute PCs
wbsimpPC<-augment(pca,wbsimp) #Add PCs to dataframe

ggplot(wbsimpPC,aes(x=IT.NET.USER.ZS,y=SH.ANM.NPRG.ZS))+geom_point()+coord_fixed()
```

IT.NET.USER.ZS = No. people using internet
SH.ANM.NPRG.ZS = Prev. anaemia non-preg.

---

# Components

```{r,echo=F,fig.height=5.5}


ggplot(wbsimpPC,aes(x=.fittedPC1,y=-.fittedPC2))+geom_point()+xlab('Principal Component 1')+ylab('Principal Component 2')+coord_fixed()
```

---

# Animation

```{r anim, echo=FALSE,animation.hook='gifski',interval=0.2}

theta<--pi/4
steps<-20
theta_inc<-seq(0,theta,length.out = steps)
for (i in 1:steps){
  rot<-matrix(c(cos(theta_inc[i]),-sin(theta_inc[i]),sin(theta_inc[i]),cos(theta_inc[i])),2,2)
  wbtmp<-mutate(wbsimpPC,c1=rot[1,1]*IT.NET.USER.ZS+rot[1,2]*SH.ANM.NPRG.ZS,c2=rot[2,1]*IT.NET.USER.ZS+rot[2,2]*SH.ANM.NPRG.ZS)
  plot(wbtmp$c1,wbtmp$c2,pch=19,xlab='',ylab='',xlim = c(-3,3),ylim = c(-3,3))
}

```

---

# Or as new coordinates

```{r,echo=F,fig.height=5.5}

ggplot(wbsimpPC,aes(x=IT.NET.USER.ZS,y=SH.ANM.NPRG.ZS))+geom_point()+coord_fixed()

```

---

# Or as new coordinates

```{r,echo=F,fig.height=5.5}

ggplot(wbsimpPC,aes(x=IT.NET.USER.ZS,y=SH.ANM.NPRG.ZS))+geom_point()+geom_abline(slope=-1,intercept = 0,col='blue')+geom_abline(slope=1,intercept = 0,col='green')+coord_fixed()


```

First PC projects onto blue line, second PC on to green line.

---

# PCA and Factor Models

- Suppose the data are generated from the following statistical model

$$\bx_i=\mathbf{A}\by_i+\boldsymbol{\xi}_i$$
--

- where
  + $\mathbf{x}_i$ is a $p\times 1$ data vector, 
  + $\mathbf{y}_i$ is a $m\times 1$ latent factor,
  + $\mathbf{A}$ are factor loadings,
  + $\boldsymbol{\xi}_i$ is a $p\times 1$ error vector.
--

- This can be estimated using PCs


---

# Summary

- PCA can be thought of as:
--

  + Compressing data with matrix decomposition.
  + Rotating the data.
  + Constructing new coordinates.
  + Projecting onto a low-dimensional hyper-plane.
  + A technique to estimate latent factors. 
--

- All of these intuitions are useful.

---

class: inverse, middle, center

# Multidimensional Scaling (MDS)

---

#The idea

- Input points: $\bx_i\in\bbR^p$ for $i=1,\dots,n$ 
- Output points: $\by_i\in\bbR^m$ for $i=1,\dots,n$
  + $m<<p$
--

- Denote distance between $\bx_i$ and $\bx_j$ as $\delta_{ij}$.
--

- Denote distance between $\by_i$ and $\by_j$ as $d_{ij}$.
--

- Want $\delta_{ij}$ to be similar to $d_{ij}$.


---

# Strain

- Assume Euclidean distances 
--
for now!
--

- Objective is to minimise strain defined as:

$$\sum\limits_{i=1}^{n-1}\sum\limits_{j=i+1}^n (\delta_{ij}^2-d_{ij}^2)$$
--

- This is known as classical MDS
--

- How can we solve this?

---

# Solution

1. Construct an $n\times n$ matrix of squared interpoint distances $\bDelta^{(2)}=\left\{\delta^2_{ij}\right\}$
--

2. Double center $\bDelta^{(2)}$ by computing $\bB=\bH'\bDelta^{(2)}\bH$ where $\bH=\bI-(1/n)\biota\biota'$
--

3. Find the eigenvalue decomposition of $\bB=\bU\bLambda\bU'$
--

4. The output coordinates are given by the first $m$ columns of $\bU\bLambda^{1/2}$

---

# Implementation

```{r,message=F,echo=T,warning=F}
library(tidyverse)
wb<-read_csv('data/WorldBankClean.csv')
wb%>%
  select_if(.,is.numeric)%>% #Use numeric data
  scale()%>% #Standardise
  dist()%>% #Compute distance matrix
  cmdscale()%>% #MDS
  as_tibble(.name_repair = 'universal')%>%
  cbind(wb)->wb_mds

```

---

# Plot

```{r,echo=F}
wb_mds%>%
  ggplot(aes(x=`...1`,y=`...2`,label=`Country Code`,text=`Country Name`))+geom_point(size=0.2)+
  geom_text(size=3,nudge_x=0.1)+xlab('Coordinate 1')+ylab('Coordinate 2')+coord_fixed()->g1
ggplotly(g1,dynamicTicks = T,tooltip="text")%>%
  frameWidget(width="100%",height="100%")  
  
```

---

# Look familiar??

- This is almost identical to Principal Components Analysis
--

- The axes have been flipped!
--

- PCA is invariant to reflections.
--
Why?
--

- MDS is also invariant to rotations.
--
Why?

---

# Why are they the same?

- Proof is a bit involved.
--

- The key idea to show that $\bB$ is related to $\bX\bX'$ while $\bS$ is related to $\bX'\bX$
--

- $\bX'\bX$ and $\bX\bX'$ have the same non-zero eigenvalues.
--

- Geometrically, the result makes sense by thinking about the extreme case where the data lie on a $m$-dimensional plane.

---
class:inverse,middle,center

# Beyond Euclidean Distance

---

# Non-Euclidean Distance

- What if a non-Euclidean distance is used?
--

- In this case classical MDS does not minimise Strain as defined previously.
--

- It has optimality properties in terms of $\bB$, minimising.

$$\sum$$
--

- Distances between output points faithfully represent distances between input points.

---

# Implementation (L1)

```{r,message=F,echo=T,warning=F}
library(tidyverse)
wb<-read_csv('data/WorldBankClean.csv')
wb%>%
  select_if(.,is.numeric)%>% #Use numeric data
  scale()%>% #Standardise
  dist(method = 'manhattan')%>% #Compute distance matrix
  cmdscale()%>% #MDS
  as_tibble(.name_repair = 'universal')%>%
  cbind(wb)->wb_mds_L1

```

---

# Plot (L1)

```{r,echo=F,fig.align='center'}
wb_mds_L1%>%
  ggplot(aes(x=`...1`,y=-`...2`,label=`Country Code`,text=`Country Name`))+geom_point(size=0.2)+
  geom_text(size=3,nudge_x = 0.1)+xlab('Coordinate 1')+ylab('Coordinate 2')+coord_fixed()
  
```

---


# Plot (L2)

```{r,echo=F,fig.align='center'}
wb_mds%>%
  ggplot(aes(x=`...1`,y=`...2`,label=`Country Code`,text=`Country Name`))+geom_point(size=0.2)+
  geom_text(size=3,nudge_x = 0.1)+xlab('Coordinate 1')+ylab('Coordinate 2')+coord_fixed()
  
```

---

# Why is this useful?

- We can have distances/dissimilarities between all sorts of objects
--
  
  + Time series
  + Functions
  + Probability distributions
  + Strings/ Texts

---

# A toy example

- Consider the word for "mother" in different languages
--

- The Levenshtein distance can be computed between strings
  + Counts number of insertions, deletions and substitutions to convert one string to another
--

- Pairwise Levenshtein distances computed and then classical multidimensional scaling applied.

---

#Languages

```{r, echo=FALSE,message=F,warning=F}
mother<-read_csv('data/mother.csv')
n<-nrow(mother)
dd<-matrix(0,n,n)
for (i in 1:n){
  for (j in 1:n){
    dd[i,j]<-adist(mother$Word[i],mother$Word[j])
    dd[j,i]<-dd[i,j]
  }
}

df<-cbind(cmdscale(as.dist(dd)),mother)
colnames(df)[1:2]<-c("Comp. 1", "Comp. 2")

ggplot(df,aes(x=`Comp. 1`,y=`Comp. 2`,label=Word,text=Language))+geom_text()->g1

ggplotly(g1,tooltip = 'text')%>%frameWidget()


```

---

# Road distances

- Suppose that we have the road distances between different cities in Australia.
--

- The road distances are non-Euclidean since roads can be quite wiggly.
--

- We want to create a 2-dimensional map with the locations of the cities using only these road distances.  
--

- Classical MDS can give an approximation that is quite close to a real map.

---

# Road Distances


```{r setd,echo=FALSE}
dm<-matrix(c(
0,0,0,0,0,0,0,0,  
1717,0,0,0,0,0,0,0,
2546,996,0,0,0,0,0,0,
3054,1674,868,0,0,0,0,0,
3143,2063,1420,728,0,0,0,0,
5954,4348,4144,3452,2724,0,0,0,
2727,3415,4000,3781,3053,4045,0,0,
2324,3012,2644,2270,1542,3630,1511,0
),
8,8,byrow = TRUE)
doz<-as.dist(dm)
attributes(doz)$Labels<-c("Cairns",
                         "Brisbane",
                         "Sydney",
                         "Melbourne",
                         "Adelaide",
                         "Perth",
                         "Darwin",
                         "Alice Springs")                  
kable(as.matrix(doz))%>% kable_styling(bootstrap_options = c("striped","hover","condensed"),font_size = 12)%>%
  scroll_box(height="500px")
```

---

# Australia


```{r ozmap,eval=TRUE,cache=TRUE, echo=FALSE,message=FALSE, warning=FALSE}
oz<-get_map(location=c(134,-23.5),zoom=4,maptype='watercolor',source='stamen')
coords<-data_frame(city=c("Sydney",
                  "Melbourne",
                  "Brisbane",
                  "Perth",
                  "Adelaide",
                  "Cairns",
                  "Alice Springs",
                  "Darwin"),
           lng=c(151.2093,
                 144.9631,
                 153.025124,
                 115.860457,
                 138.600746,
                 145.7781,
                 133.8807,
                 130.845642),
           lat=c(-33.8688,
                 -37.8136,
                 -27.469771,
                 -31.950527,
                 -34.928499,
                 -16.9186,
                 -23.6980,
                 -12.463440)
           )
g<-ggmap(oz)+geom_text(data=coords,aes(x=lng, y=lat,label=city),size=8,nudge_y = 1.5)+geom_point(data=coords,aes(x=lng, y=lat),size=6)
g
```

---

# MDS Solution

```{r,mdsoz,echo=FALSE}
cmdscale(doz)->dozout
colnames(dozout)<-c('lng','lat')
dozout%>%
  as.data.frame()%>%
  rownames_to_column("city")->
  ozmdsout
ggplot(ozmdsout,aes(x=lng,y=lat,label=city))+geom_text(size=8)+coord_cartesian(xlim=c(-3500,3000))
```

---

# Rotate

```{r,mdsoz2,echo=FALSE,eval=TRUE}
cmdscale(doz)->dozout
th<-0.65
dozout<-dozout/130
rot<-matrix(c(cos(th),-sin(th),sin(th),cos(th)),2,2)
dozout<-dozout%*%rot
cent<-apply(coords[,2:3],2,mean)+c(1,-1)
for (i in 1:8){dozout[i,]<-dozout[i,]+cent}
colnames(dozout)<-c('lng','lat')
dozout%>%
  as.data.frame()%>%
  rownames_to_column("city")->
  ozmdsout
ggplot(ozmdsout,aes(x=lng,y=lat,label=city))+geom_text(size=8)+coord_cartesian(xlim=c(115,156))
```
---

# Back with Map

```{r, mdssolmap,echo=FALSE,eval=TRUE}
ggmap(oz)+geom_text(data=ozmdsout,aes(x=lng, y=lat,label=city),size=8,nudge_y = 1.5)+geom_point(data=ozmdsout,aes(x=lng, y=lat),size=6)
```


---
class: inverse, middle, center

# MDS: Beyond Linearity

---

# A different objective

