---
title: "AMSI Winter </br> School 2021"
subtitle: "Dimension Reduction"
author: "Anastasios Panagiotelis"
institute: "University of Sydney"
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css: [default,"css/mtheme.css","css/mod.css"]
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    includes:
      before_body: aux/defs.html
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(gifski)
```

---

class: center, middle, inverse

# Motivation

---

# Finding structure

- Examples
- Simple structure from high-dimensional data

---

class: center, middle, inverse

# Principal Components Analysis

---

# Explaining Variance

- Let there be $n$ observations of $p$ variables; $x_{ij}$ denotes observation $i$ and variable $j$.
--

- Find some linear combination of variables that has maximal variance.
--

- Find $w_1,w_2,\dots,w_p$ such that

$$y_i=w_1x_{i1}+w_2x_{i2}+\dots w_px_{ip}$$
has the biggest possible variance.
--
This is called the first principal component (PC).


---

# More PCs

- After finding the first principal component can look for a linear combination that
--

  + Has maximum variance
  + Is uncorrelated with the first PC
--

- This is called the second principal component
--

- This continues until there are as many PCs as variables.


---


# No cheating...

- Arbitrarily big weights
--
$\rightarrow$ arbitrarily big variance.
--

  + Constrain $\sum w_j=1$
--

- Sensitive to units of measurement.
--

  + Center all variables by subtracting the mean.
  + Standardise all variables to have unit variance.
  
---

# An example

```{r,message=F,echo=F}

library(tidyverse)

wb<-read_csv('data/WorldBank.csv',na = '..',n_max = 43617)

wb%>%
  filter(!(`Country Name`%in%c('India','China')))%>%
  select(`Country Name`,`Country Code`,Series=`Series Code`,Value=`2015 [YR2015]`)%>%
  pivot_wider(id_cols = 1:2,names_from = Series,values_from=Value)->wb_rs

mis<-apply(wb_rs,2,function(x){sum(is.na(x))})

k<-50

wb_sel<-wb_rs[,(mis<k)]

wb_clean<-wb_sel[complete.cases(wb_sel),]

write_csv(wb_clean,'data/WorldBankClean.csv')

```

- Data obtained from the World Bank for 132 countries and 67 variables.
--

- Variables include:
--

  + Health indicators and life expectancy
  + Technology adoption
  + Education rates
  + Economic indicators 
--

- Some variables and observations removed to reduce missing data.

---

# Data

```{r,message=F,echo=F}
library(knitr)
library(kableExtra)
wb<-read_csv('data/WorldBankClean.csv')
kable(wb,format = 'html')%>%
  kable_styling(font_size = 9,bootstrap_options = c("striped","hover","condensed"))%>%
  scroll_box(height="400px")

```

For more on data LINK

---

# Implementation

R Code to implement PCA
--

```{r,message=F,echo=T}
library(tidyverse)
library(broom)
wb<-read_csv('data/WorldBankClean.csv')
wb%>%
  select_if(.,is.numeric)%>% #Use numeric data
  scale()%>% #Standardise
  prcomp()->pca #Compute PCs
wbPC<-augment(pca,wb) #Add PCs to dataframe

```

---

# Scree plot

```{r, echo=F}
tibble(Variance=pca$sdev^2,Component=1:length(pca$sdev))%>%
  ggplot(aes(x=Component,y=Variance))+geom_line()+geom_hline(yintercept=1,col='blue')
```

---

# Plot

```{r,message=F,echo=F,eval=T,message=F,warning=F}
library(plotly)
library(widgetframe)

wbPC%>%
  ggplot(aes(x=.fittedPC1,y=.fittedPC2,label=`Country Code`,text=`Country Name`))+geom_point(size=0.2)+
  geom_text(size=3,nudge_x=0.1)+xlab('Principal Component 1')+ylab('Principal Component 2')->g1

ggplotly(g1,dynamicTicks = T,tooltip="text")%>%
  frameWidget(width="100%",height="100%")

```

---

# Uncovering Structure

- Countries towards the right tend to be more economically developed.
--

- Countries towards the bottom tend to be larger in population.
--

- Countries that are similar to one another are closer together on the plot.
--

- A small number of $m<<p$ PCs is sufficient to explain a large proportion of variance.

---
class: middle, center, inverse

# PCA: The Algebra

---

# PCA as optimisation

- LC given by $\by=\bX\bw$
--

- Variance of LC: $\frac{1}{n-1}\sum_{i=1}^n y^2_i=\frac{1}{n-1}\by'\by$
--

- Optimisation problem is
$$\underset{\bw}{argmax}\,\frac{1}{n-1}\bw'\bX'\bX\bw$$

subject to $\bw'\bw=1$
--

- Replace $\bS=\frac{1}{n-1}\bX'\bX$
---

# Solution

- Lagrangian is

$$\calL=\bw'\bS\bw-\lambda(\bw'\bw-1)$$
--

- First order conditions

$$\frac{\partial\calL}{\partial{\bw}}=2\bS\bw-2\lambda\bw$$
--

- Need to find $\mathbf{w}$ to satisfy

$$\bS\bw=\lambda\bw$$
---

# Eigenvalue Decomposition

- Solutions are given by the eigenvalue decomposition.
--

- There are multiple solutions. The eigenvector corresponding to the largest eigenvalue gives the weights of the first principal component.
--

- The eigenvector corresponding the the second largest eigenvalue gives the weights of the second principal component.
--

- And so on...

---

# Data compression

- For the eigendecomposition

$$\bS=\sum_{j=1}^p \lambda_j\bw_j\bw_j'$$
- This can be approximated by

$$\bS\approx\sum_{j=1}^{\color{blue}{m}} \lambda_j\bw_j\bw_j'$$
---

class: inverse, middle, center

# PCA: The geometry

---

# Rotations

- A matrix of eigenvectors $\bW$ is a rotation matrix
--
  
  + Columns/rows are orthogonal
  + Columns/rows have unit length
--

- Multiplying a vector by rotation matrix literally rotates that vector.

---

# Rotation is PCA

- Principal components given by $\bY=\bX\bW$
--

- Each observation (row of $\bX$) is rotated to new components
--

- This is best seen with a simple example

---

# A simple case

```{r,echo=F,fig.height=5.5}


wb%>%
  select(IT.NET.USER.ZS,SH.ANM.NPRG.ZS)%>%
  scale()->wbsimp #Use numeric data
wbsimp%>% #Standardise
  prcomp()->pca #Compute PCs
wbsimpPC<-augment(pca,wbsimp) #Add PCs to dataframe

ggplot(wbsimpPC,aes(x=IT.NET.USER.ZS,y=SH.ANM.NPRG.ZS))+geom_point()
```

IT.NET.USER.ZS = No. people using internet
SH.ANM.NPRG.ZS = Prev. anaemia non-preg.

---

# Components

```{r,echo=F,fig.height=5.5}


ggplot(wbsimpPC,aes(x=.fittedPC1,y=-.fittedPC2))+geom_point()+xlab('Principal Component 1')+ylab('Principal Component 2')
```

---

# Animation

```{r anim, echo=FALSE,animation.hook='gifski',interval=0.2}

theta<--pi/4
steps<-20
theta_inc<-seq(0,theta,length.out = steps)
for (i in 1:steps){
  rot<-matrix(c(cos(theta_inc[i]),-sin(theta_inc[i]),sin(theta_inc[i]),cos(theta_inc[i])),2,2)
  wbtmp<-mutate(wbsimpPC,c1=rot[1,1]*IT.NET.USER.ZS+rot[1,2]*SH.ANM.NPRG.ZS,c2=rot[2,1]*IT.NET.USER.ZS+rot[2,2]*SH.ANM.NPRG.ZS)
  plot(wbtmp$c1,wbtmp$c2,pch=19,xlab='',ylab='',xlim = c(-3,3),ylim = c(-3,3))
}

```

---

# Or as new coordinates

```{r,echo=F,fig.height=5.5}

ggplot(wbsimpPC,aes(x=IT.NET.USER.ZS,y=SH.ANM.NPRG.ZS))+geom_point()+coord_fixed()

```

---

# Or as new coordinates

```{r,echo=F,fig.height=5.5}

ggplot(wbsimpPC,aes(x=IT.NET.USER.ZS,y=SH.ANM.NPRG.ZS))+geom_point()+geom_abline(slope=-1,intercept = 0,col='blue')+geom_abline(slope=1,intercept = 0,col='green')+coord_fixed()


```

First PC projects onto blue line, second PC on to green line.

---

# PCA and Factor Models

- Suppose the data are generated from the following statistical model

$$\bx_i=\mathbf{A}\by_i+\boldsymbol{\xi}_i$$
--

- where
  + $\mathbf{x}_i$ is a $p\times 1$ data vector, 
  + $\mathbf{y}_i$ is a $m\times 1$ latent factor,
  + $\mathbf{A}$ are factor loadings,
  + $\boldsymbol{\xi}_i$ is a $p\times 1$ error vector.
--

- This can be estimated using PCs


---

# Summary

- PCA can be thought of as:
--

  + Compressing data with matrix decomposition.
  + Rotating the data.
  + Constructing new coordinates.
  + Projecting onto a low-dimensional hyper-plane.
  + A technique to estimate latent factors. 
--

- All of these intuitions are useful.

---

class: inverse, middle, center

# Multidimensional Scaling (MDS)

---

#The idea

- Input points: $\bx_i\in\bbR^p$ for $i=1,\dots,n$ 
- Output points: $\by_i\in\bbR^m$ for $i=1,\dots,n$
  + $m<<p$
--

- Denote distance between $\bx_i$ and $\bx_j$ as $\delta_{ij}$.
--

- Denote distance between $\by_i$ and $\by_j$ as $d_{ij}$.
--

- Want $\delta_{ij}$ to be similar to $d_{ij}$.


---

# Strain

- Assume Euclidean distances 
--
for now!
--

- Objective is to minimise strain defined as:

$$\sum\limits_{i=1}^{n-1}\sum\limits_{j=i+1}^n (\delta_{ij}^2-d_{ij}^2)$$
--

- How can we solve this?

---

# Solution

1. Double Center
2. Eigenvalue Decomposition

---