---
title: "Dimension Reduction:</br>PCA"
author: "Anastasios Panagiotelis"
institute: "University of Sydney"
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css: [default,"../css/mtheme.css","../css/mod.css"]
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    includes:
      before_body: ../auxil/defs.html
---

# Outline

- What is PCA?
--

- Application of PCA
--

- Algebraic understanding
--

- Geometric understanding
--

- Latent factor model understanding


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.align='center')
library(gifski)
library(ggrepel)
library(ggmap)
```

---

class: center, middle, inverse

# Principal Components Analysis

---

# Explaining Variance

- Let there be $n$ observations of $p$ variables; $x_{ij}$ denotes observation $i$ and variable $j$.
--

- Find some linear combination of variables that has maximal variance.
--

- Find $w_1,w_2,\dots,w_p$ such that

$$y_i=w_1x_{i1}+w_2x_{i2}+\dots w_px_{ip}$$
has the biggest possible variance.
--

- This is the first principal component (PC).


---

# More PCs

- After finding the first principal component can look for a linear combination that
--

  + Has maximum variance
  + Is uncorrelated with the first PC
--

- This is called the second principal component
--

- This continues until there are as many PCs as variables.


---


# No cheating...

- Arbitrarily big weights
--
$\rightarrow$ arbitrarily big variance.
--

  + Constrain $\sum w^2_j=1$
--

- Sensitive to units of measurement.
--

  + Center all variables by subtracting the mean.
  + Standardise all variables to have unit variance.
  

```{r,message=F,echo=F}

library(tidyverse)

wb<-read_csv('../data/WorldBank.csv',na = '..',n_max = 43617)

wb%>%
  filter(!(`Country Name`%in%c('India','China')))%>%
  select(`Country Name`,`Country Code`,Series=`Series Code`,Value=`2015 [YR2015]`)%>%
  pivot_wider(id_cols = 1:2,names_from = Series,values_from=Value)->wb_rs

mis<-apply(wb_rs,2,function(x){sum(is.na(x))})

k<-50

wb_sel<-wb_rs[,(mis<k)]

wb_clean<-wb_sel[complete.cases(wb_sel),]

write_csv(wb_clean,'../data/WorldBankClean.csv')

```


---
class: center, middle, inverse

#An application

---

# Implementation

R Code to implement PCA for World Bank Data
--

```{r,message=F,echo=T}
library(tidyverse)
library(broom)
wb<-read_csv('../data/WorldBankClean.csv')
wb%>%
  select_if(.,is.numeric)%>% #Use numeric data
  scale()%>% #Standardise
  prcomp()->pca #Compute PCs
wbPC<-augment(pca,wb) #Add PCs to dataframe

```

---

# Explaining variance

- The variance of the first PC is `r (pca$sdev[1]^2)%>%round(2)`.
--
  
  + This represents `r (100*(pca$sdev[1]^2)/length(pca$sdev))%>%round(2)`% of the total variance of the data.
--

- The variance of the second PC is `r (pca$sdev[2]^2)%>%round(2)`.
--
  
  + This represents `r (100*(pca$sdev[2]^2)/length(pca$sdev))%>%round(2)`% of the total variance of the data.
--

- Together the first 5 PCs represent `r (100*sum(pca$sdev[1:5]^2)/length(pca$sdev))%>%round(2)`% of the total variance of the data.

---

# Scree plot

```{r, echo=F}
tibble(Variance=pca$sdev^2,Component=1:length(pca$sdev))%>%
  ggplot(aes(x=Component,y=Variance))+geom_line()+geom_hline(yintercept=1,col='blue')
```

---

# Plot

```{r,message=F,echo=F,eval=T,message=F,warning=F}
library(plotly)
library(widgetframe)

wbPC%>%
  ggplot(aes(x=.fittedPC1,y=.fittedPC2,label=`Country Code`,text=`Country Name`))+geom_point(size=0.2)+
  geom_text(size=3,nudge_x=0.1)+xlab('Principal Component 1')+ylab('Principal Component 2')+coord_fixed()->g1

ggplotly(g1,dynamicTicks = T,tooltip="text")%>%
  frameWidget(width="100%",height="100%")

```

---

# Uncovering Structure

- Countries towards the right tend to be more economically developed.
--

- Countries towards the bottom tend to be larger in population.
--

- Countries that are similar to one another are closer together on the plot.
--

- A small number of PCs explains a large proportion of variance.

---
class: middle, center, inverse

# PCA: The Algebra

---

# PCA as optimisation

- LC given by $\by=\bX\bw$
--

- Variance of LC: $\frac{1}{n-1}\sum_{i=1}^n y^2_i=\frac{1}{n-1}\by'\by$
--

- Optimisation problem is
$$\underset{\bw}{\max}\,\frac{1}{n-1}\bw'\bX'\bX\bw$$

subject to $\bw'\bw=1$
--

- Substitute $\bS=\frac{1}{n-1}\bX'\bX$
---

# Solution

- Lagrangian is

$$\calL=\bw'\bS\bw-\lambda(\bw'\bw-1)$$
--

- A first order condition is

$$\frac{\partial\calL}{\partial{\bw}}=2\bS\bw-2\lambda\bw$$
--

- Need to find $\mathbf{w}$ to satisfy

$$\bS\bw=\lambda\bw$$
---

# Eigenvalue Decomposition

- Solutions are given by the eigenvalue decomposition.
--

- There are multiple solutions. The eigenvector corresponding to the largest eigenvalue gives the weights of the first principal component.
--

- The eigenvector corresponding the the second largest eigenvalue gives the weights of the second principal component.
--

- And so on...

---

# Data compression

- When $\lambda_j$ / $\bw_j$ are eigenvalues/eigenvectors

$$\bS=\sum_{j=1}^p \lambda_j\bw_j\bw_j'$$
- This can be approximated by

$$\bS\approx\sum_{j=1}^{\color{blue}{m}} \lambda_j\bw_j\bw_j'$$
---

class: inverse, middle, center

# PCA: The geometry

---

# Rotations

- For symmetric p.s.d matrices, the matrix of eigenvectors $\bW$ is a rotation matrix
--
  
  + Columns/rows are orthogonal
  + Columns/rows have unit length
--

- Multiplying a vector by rotation matrix literally rotates that vector.

---

# Rotation is PCA

- Principal components given by $\bY=\bX\bW$
--

- Each observation (row of $\bX$) is rotated to new components
--

- This is best seen with a simple example

---

# A simple case

```{r,echo=F,fig.height=5.5}


wb%>%
  select(IT.NET.USER.ZS,SH.ANM.NPRG.ZS)%>%
  scale()->wbsimp #Use numeric data
wbsimp%>% #Standardise
  prcomp()->pca #Compute PCs
wbsimpPC<-augment(pca,wbsimp) #Add PCs to dataframe

ggplot(wbsimpPC,aes(x=IT.NET.USER.ZS,y=SH.ANM.NPRG.ZS))+geom_point()+coord_fixed()
```

IT.NET.USER.ZS = No. people using internet
SH.ANM.NPRG.ZS = Prev. anaemia non-preg.

---

# Components

```{r,echo=F,fig.height=5.5}


ggplot(wbsimpPC,aes(x=.fittedPC1,y=-.fittedPC2))+geom_point()+xlab('Principal Component 1')+ylab('Principal Component 2')+coord_fixed()
```

---

# Animation

```{r anim, echo=FALSE,animation.hook='gifski',interval=0.2}

theta<--pi/4
steps<-20
theta_inc<-seq(0,theta,length.out = steps)
for (i in 1:steps){
  rot<-matrix(c(cos(theta_inc[i]),-sin(theta_inc[i]),sin(theta_inc[i]),cos(theta_inc[i])),2,2)
  wbtmp<-mutate(wbsimpPC,c1=rot[1,1]*IT.NET.USER.ZS+rot[1,2]*SH.ANM.NPRG.ZS,c2=rot[2,1]*IT.NET.USER.ZS+rot[2,2]*SH.ANM.NPRG.ZS)
  plot(wbtmp$c1,wbtmp$c2,pch=19,xlab='',ylab='',xlim = c(-3,3),ylim = c(-3,3))
}

```

---

# Or as new coordinates

```{r,echo=F,fig.height=5.5}

ggplot(wbsimpPC,aes(x=IT.NET.USER.ZS,y=SH.ANM.NPRG.ZS))+geom_point()+coord_fixed()

```

---

# Or as new coordinates

```{r,echo=F,fig.height=5.5}

ggplot(wbsimpPC,aes(x=IT.NET.USER.ZS,y=SH.ANM.NPRG.ZS))+geom_point()+geom_abline(slope=-1,intercept = 0,col="#E69F00")+geom_abline(slope=1,intercept = 0,col="#56B4E9")+coord_fixed()


```

First PC projects onto orange line, second PC on to blue line.

---

# PCA and Factor Models

- Suppose the data are generated from the following statistical model

$$\bx_i=\bA\by_i+\boldsymbol{\xi}_i$$
--

- where
  + $\bx_i$ is a $p\times 1$ data vector, 
  + $\by_i$ is a $m\times 1$ latent factor,
  + $\bA$ are factor loadings,
  + $\boldsymbol{\xi}_i$ is a $p\times 1$ error vector.
--

- The $\by_i$ can be estimated using PCs


---

# Summary

- PCA can be thought of as:
--

  + Compressing data with matrix decomposition.
  + Rotating the data.
  + Constructing new coordinates.
  + Projecting onto a low-dimensional hyper-plane.
  + A technique to estimate latent factors. 
--

- All of these intuitions are useful.

---

class: inverse, center, middle

#Questions?