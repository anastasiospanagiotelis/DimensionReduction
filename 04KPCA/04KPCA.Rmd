---
title: "AMSI Winter </br> School 2021"
subtitle: "Dimension Reduction:</br>Kernel PCA"
author: "Anastasios Panagiotelis"
institute: "University of Sydney"
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css: [default,"../css/mtheme.css","../css/mod.css"]
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    includes:
      before_body: ../aux/defs.html
---

# Outline

- Feature mapping
--

  + Why we sometimes need to go to higher dimensions to get to lower dimensions.
--

- Kernel trick
--

  + How we can compute inner products of high (even infinite) dimensional transformations of the data using only our original data
--

- Kernel PCA algorithm
--

  + Using this for PCA.


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.align='center',echo=FALSE,message = F,warning = F)
library(gifski)
library(ggrepel)
library(ggmap)
library(tidyverse)
library(ggthemes)
library(broom)
library(rgl)
library(plotly)
knitr::knit_hooks$set(webgl = hook_webgl)
```

---

class: center, middle, inverse

# Feature Mapping

---

# Non linear PCA

- Regular PCA gives principal components that are linear functions of the data.
--

- As a rough idea consider including polynomials $x^2, x^3,x^4,\dots$ as well as cross products.
--

- In general, let $\Phi:\bbR^m\rightarrow\bbR^M$ where $P>p$ be a feature map.
--

- Rather than carry out principal components on $\bx$, carry out principal components on $\Phi(\bx)$

---

# Question

- Aren't we doing dimension reduction?
--

- Yes, however the idea is that a linear transformation can only reveal patterns if we transform to a higher dimension first.
--

- Consider this example

---

# Circles

```{r}
set.seed(1)
pts<-200
theta<-runif(pts)*2*pi
r<-c(runif(pts/2),runif(pts/2)+1.5)
x<-cos(theta)*r
y<-sin(theta)*r

df<-tibble(x1=x,x2=y,group=c(rep('Black',pts/2),rep('Yellow',pts/2)))
df%>%
  ggplot(aes(x=x1,y=x2,col=group))+geom_point()+scale_color_colorblind()

```

---

# Circles example

- Suppose we use PCA to go from 2 dimensions to 1 dimension.
--

- Since PCA can only rotate the data, there is no way to separate out the black and yellow points.
--

- This may be an important feature that we want to identify in lower dimensional space.

---

# First PC of circles example


```{r}
df%>%
  select_if(is.numeric)%>%
  prcomp()%>%
  tidy%>%
  filter(PC==1)%>%
  `$`(value)->pc1

add_column(df,pc1=pc1)%>%
  ggplot(aes(x=pc1,y=0,col=group))+geom_point()+scale_color_colorblind()

```

---

# Add dimension

- Consider adding a new variable given by $x_3=x_1^2+x_2^2$
--

- Then we we carry out PCA on $x_1$, $x_2$ and $x_3$
--

- Will the first principal component separate out the black and yellow points.

---

# More dimensions

```{r,webgl=T}
df<-mutate(df,x3=x1^2+x2^2)
#library(car)
invisible(open3d())
plot3d(x=df$x1,z=df$x2,y=df$x3,xlab='x1',ylab='x2',zlab='x3',col=c(rep('black',100),rep('#E69F00',100)))
rglwidget()
close3d()
```

---

# First PC with extra feature

```{r}
df%>%
  select_if(is.numeric)%>%
  prcomp()%>%
  tidy%>%
  filter(PC==1)%>%
  `$`(value)->pc1

add_column(df,pc1b=pc1)%>%
  ggplot(aes(x=pc1,y=0,col=group))+geom_point()+scale_color_colorblind()

```

---

# Problems

- How to compute feature space?
--

- Will this be computationally feasible?
--

- There is a trick we can use so that we never have to compute the feature space
--

- This is known as the 'kernel' trick and is pervasive in machine learning.

---

class: inverse, middle, center

# The kernel trick

---

# The kernel trick

- Consider $\Phi(\bx)$ which is an $P$ dimensional vector.
--

- In many algorithms (including PCA) the solution can be reworked in terms of $\langle\Phi(\bx),\Phi(\bz)\rangle$ (or if you prefer $\Phi(\bx)'\Phi(\bz)$).
--

- The inner product in the feature space, can be written as a function of $\bx$ and $\bz$, known as the kernel
$$\langle\Phi(\bx),\Phi(\bz)\rangle=K(\bx,\bz)$$


---

# An example

- Consider the mapping $\Phi(\bx)=\left(x_1^2,\dots,x_p^2,\right.$--
$\sqrt{2}x_1x_2,\dots,\sqrt{2}x_{p-1}x_p,$--
$\sqrt{2}x_1,\dots,\sqrt{2}x_p,$--
$\left.1\right)$
--

- We have gone from an $p$-dimensional vector to a $P=(p(p+3))/2)+1$ dimensional vector.
--

- The kernel function is

$$K(\bx,\bz)=(\langle\bx,\bz\rangle+1)^2=\langle\Phi(\bx),\Phi(\bz)\rangle$$
--

- This is called a polynomial kernel.

---

# Other kernels

- Other kernels are available
--

  + RBF kernel
  + Hyperbolic tangent kernel
--

- These may provide the inner product for infinite dimensional feature spaces.
--

- The feature space may not be unique.

---

class: inverse, middle, center

# Kernel PCA

---

# A PCA refresher

- The standard way to do PCA is to solve the eigenvalue problem
--

$$\bC\bv=\lambda\bv$$
--

- This is equivalent to
--

$$\frac{1}{n}\bX'\bX\bv=\lambda\bv$$
- The principal components are given by $\bX\bv$ (not by $\bv$).

---

# A silly way to do PCA

- Take
--

$$\frac{1}{n}\bX'\bX\bv=\lambda\bv$$
--

- Pre-multiply by $\bX$ and rearrange
--

$$\bX\bX'\bX\bv=n\lambda\bX\bv$$
- Replace $\bX\bv$ with $\by$ and $n\lambda$ with $\tilde{\lambda}$
--

$$\bX\bX'\by=\tilde{\lambda}\by$$

---

# Kernel matrix

-  Principal components could be found by finding eigenvectors of $\bX\bX'$ instead of $\frac{1}{n}\bX'\bX$.
--

- Normally you wouldn't do this since $\bX\bX'$ will be a bigger matrix than $\bX'\bX$
--

- Unless...
--
rather than $\bX$ with rows $\bx_i$ you have a matrix with rows $\Phi(\bx_i)$.
--

 + Then $\bX\bX'$ becomes $\bK$.
--

 + Here $\bK$ is the **kernel matrix** an $n\times n$ matrix with elements $K(\bx_i,\bx_j)$.
 
---

# Some caveats

- That is a simpler treatment than what you will find in many references.
--

- A more formal treatment has to deal more carefully with the possible infinite dimensionality of the feature space.
--

- Care must be taken to center $\bK$ by pre and post multiplying by the centering matrix $\bI-n^{-1}\iota\iota'$  
--

- Some sources will rescale the eigenvectors of $\bK$ by $n$ or $\lambda$.

---

# Demonstration

- Recall the world bank data.
--

- This data was analysed using PCA.
--

- On the following slide, the PCA output will be shown again
--

- After that we will work through applying kernel PCA to the same dataset.

---

# Standard PCA


```{r,message=FALSE}
library(tidyverse)
library(dimRed)
library(ggrepel)
read_csv('../data/WorldBankClean.csv')%>%
  mutate_if(is.numeric,scale)%>%
  as.dimRedData(`Country Name` + `Country Code`~.,data=.)->wb
  
pcaout <- embed(.data = wb, .method="PCA")

df<-tibble(cbind(pcaout@data@meta,pcaout@data@data))


ggplot(df,aes(x=PC1,y=PC2,label=`Country Code`))+geom_text(size=2)

```

---

# The dimRed package

- The [`dimRed` package](https://github.com/gdkrmr/dimRed) provides a unified framework for many dimension reduction techniques.
--

- It uses an S4 class for handling data with two slots
--

  + The slot `data` contains data (measures of development)
  + The slot `meta` contains other information (country names)

---

# Kernel PCA in dimRed

- By default dimRed uses the radial basis kernel
--

$$K(\bx,\bz)=\exp\left(-\frac{||\bx-\bz||^2}{2\sigma^2}\right)$$
--

- The default value of the tuning parameter is $\sigma=0.1$

---

# Code

```{r,message=FALSE,eval=F,echo=T}
library(tidyverse)
library(dimRed)
read_csv('../data/WorldBankClean.csv')%>% #Read Data
  mutate_if(is.numeric,scale)%>% #Scale Data
  as.dimRedData(`Country Name` + `Country Code`~.,data=.)->wb #Convert to S4 class
  
kpcaout <- embed(.data = wb, .method="kPCA")

df<-tibble(cbind(kpcaout@data@meta,kpcaout@data@data)) # Convert back to a dataframe

ggplot(df,aes(x=kPCA1,y=kPCA2,label=`Country Code`))+geom_text(size=2) #Plot

```

---

# Kernel PCA

```{r,message=FALSE}

kpcaout <- embed(.data = wb, .method="kPCA",kpar=list(sigma=0.1))

df<-tibble(cbind(kpcaout@data@meta,kpcaout@data@data))

ggplot(df,aes(x=kPCA1,y=kPCA2,label=`Country Code`))+geom_text(size=2)

```

---

# With different tuning parameter

```{r,echo=T}
kpcaout <- embed(.data = wb, .method="kPCA",kpar=list(sigma=0.001))
```

```{r,echo=F,fig.height=6}
df<-tibble(cbind(kpcaout@data@meta,kpcaout@data@data))
ggplot(df,aes(x=kPCA1,y=kPCA2,label=`Country Code`))+geom_text(size=2)
```

---

# With different kernel

```{r,echo=T}
kpcaout <- embed(.data = wb, .method="kPCA",kernel='tanhdot',kpar=list(scale=1))
```

```{r,echo=F,fig.height=6}
df<-tibble(cbind(kpcaout@data@meta,kpcaout@data@data))
ggplot(df,aes(x=kPCA1,y=kPCA2,label=`Country Code`))+geom_text(size=2)
```

---


# Image example

- Read in png files of images of the letter 'A' rotated and rescaled.
--

- There are 124848 variables.
--

- Only 10898 of these variable have any variation across the images.
--

- Lesson: Don't try a complicated dimension reduction technique when a simple one is better.
--

- For these variables run PCA and kPCA.


---

# PCA

```{r,cache=TRUE}
library(png)

filenames<-list.files('../data/images/')
imagedat<-matrix(0,length(filenames), 124848)
for(i in 1:length(filenames)){
  out<-readPNG(paste0('../data/images/',filenames[i]))%>%as.vector()
  imagedat[i,]<-out
}

colnames(imagedat)<-paste0('V',str_pad(1:124848,6,'left','0'))
pix<-as_tibble(imagedat)
vars<-apply(pix,2,var)

pix<-pix[,(vars>0)]
pix<-add_column(pix,Image=filenames,.before = 1)

library(dimRed)

im<-as.dimRedData(Image~.,data=pix)


pcaout<-embed(.data = im, .method="PCA")
dfpca<-tibble(cbind(pcaout@data@meta,pcaout@data@data))
set.seed(9)

ann<-filter(dfpca,(PC2>quantile(dfpca$PC2,0.7)),(PC1>quantile(dfpca$PC1,0.7)))%>%
  sample_n(1)

ann<-rbind(ann,filter(dfpca,(PC2<quantile(dfpca$PC2,0.3)),(PC1>quantile(dfpca$PC1,0.7)))%>%
  sample_n(1))

ann<-rbind(ann,filter(dfpca,(PC2>quantile(dfpca$PC2,0.9)),(PC1<quantile(dfpca$PC1,0.2)))%>%
  sample_n(1))

ann<-rbind(ann,filter(dfpca,(PC2<quantile(dfpca$PC2,0.1)),(PC1<quantile(dfpca$PC1,0.2)))%>%
  sample_n(1))
  

sdx<-0.05*(max(dfpca$PC1)-min(dfpca$PC1))
sdy<-0.05*(max(dfpca$PC2)-min(dfpca$PC2))

ggplot(dfpca,aes(x=PC1,y=PC2,label=Image))+
  geom_point(size=1)+coord_equal()+
  annotation_raster(readPNG(paste0('../data/images/',ann$Image[1])),ymin = ann$PC2[1]-sdy,ymax= ann$PC2[1]+sdy,xmin = ann$PC1[1]-sdx,xmax = ann$PC1[1]+sdx)+
  annotation_raster(readPNG(paste0('../data/images/',ann$Image[2])),ymin = ann$PC2[2]-sdy,ymax= ann$PC2[2]+sdy,xmin = ann$PC1[2]-sdx,xmax = ann$PC1[2]+sdx)+
  annotation_raster(readPNG(paste0('../data/images/',ann$Image[3])),ymin = ann$PC2[3]-sdy,ymax= ann$PC2[3]+sdy,xmin = ann$PC1[3]-sdx,xmax = ann$PC1[3]+sdx)+
  annotation_raster(readPNG(paste0('../data/images/',ann$Image[4])),ymin = ann$PC2[4]-sdy,ymax= ann$PC2[4]+sdy,xmin = ann$PC1[4]-sdx,xmax = ann$PC1[4]+sdx)
  

```

---

# Kernel PCA

```{r,cache=T}
kpcaout<-embed(.data = im, .method="kPCA",kpar=list(sigma=0.0018))
dfpca<-tibble(cbind(kpcaout@data@meta,kpcaout@data@data))

set.seed(2)


ann<-filter(dfpca,(kPCA2>quantile(dfpca$kPCA2,0.8)),(kPCA1>quantile(dfpca$kPCA1,0.1)))%>%
  sample_n(1)

ann<-rbind(ann,filter(dfpca,(kPCA2<quantile(dfpca$kPCA2,0.3)),(kPCA1>quantile(dfpca$kPCA1,0.7)))%>%
  sample_n(1))

ann<-rbind(ann,filter(dfpca,(kPCA2>quantile(dfpca$kPCA2,0.9)),(kPCA1<quantile(dfpca$kPCA1,1)))%>%
  sample_n(1))

ann<-rbind(ann,filter(dfpca,(kPCA2<quantile(dfpca$kPCA2,0.3)),(kPCA1<quantile(dfpca$kPCA1,0.3)))%>%
  sample_n(1))
  

sdx<-0.05*(max(dfpca$kPCA1)-min(dfpca$kPCA1))
sdy<-0.08*(max(dfpca$kPCA2)-min(dfpca$kPCA2))

ggplot(dfpca,aes(x=kPCA1,y=kPCA2,label=Image))+
   geom_point(size=1)+coord_equal()+
  annotation_raster(readPNG(paste0('../data/images/',ann$Image[1])),ymin = ann$kPCA2[1]-sdy,ymax= ann$kPCA2[1]+sdy,xmin = ann$kPCA1[1]-sdx,xmax = ann$kPCA1[1]+sdx)+
  annotation_raster(readPNG(paste0('../data/images/',ann$Image[2])),ymin = ann$kPCA2[2]-sdy,ymax= ann$kPCA2[2]+sdy,xmin = ann$kPCA1[2]-sdx,xmax = ann$kPCA1[2]+sdx)+
  annotation_raster(readPNG(paste0('../data/images/',ann$Image[3])),ymin = ann$kPCA2[3]-sdy,ymax= ann$kPCA2[3]+sdy,xmin = ann$kPCA1[3]-sdx,xmax = ann$kPCA1[3]+sdx)+
  annotation_raster(readPNG(paste0('../data/images/',ann$Image[4])),ymin = ann$kPCA2[4]-sdy,ymax= ann$kPCA2[4]+sdy,xmin = ann$kPCA1[4]-sdx,xmax = ann$kPCA1[4]+sdx)
  

```

---

# Conclusion

- Kernel PCA has the advantage of being a non-linear dimension reduction technique.
--

- Some disadvantages include:
--

  + Need to choose a kernel.
  + Need to choose tuning parameters for the kernel.
  + Eigendecomposition is slow for large $n$, although this can be mitigated by using a kernel that imposes sparsity.

  
---

class: center, inverse, middle

# Questions?