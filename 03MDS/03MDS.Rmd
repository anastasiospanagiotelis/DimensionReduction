---
title: "AMSI Winter </br> School 2021"
subtitle: "Dimension Reduction:</br>MDS"
author: "Anastasios Panagiotelis"
institute: "University of Sydney"
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css: [default,"../css/mtheme.css","../css/mod.css"]
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    includes:
      before_body: ../aux/defs.html
---

# Outline

- Classical Multidimensional Scaling
--

  + Euclidean distance
  + Non-Euclidean distance
--

- Some applications
--

- Sammon Mapping 


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.align='center')
library(gifski)
library(ggrepel)
library(ggmap)
```

---

class: inverse, middle, center

# Multidimensional Scaling (MDS)

---

#The idea

- Input points: $\bx_i\in\bbR^p$ for $i=1,\dots,n$ 
- Output points: $\by_i\in\bbR^m$ for $i=1,\dots,n$
  + $m<<p$
--

- Denote distance between $\bx_i$ and $\bx_j$ as $\delta_{ij}$.
--

- Denote distance between $\by_i$ and $\by_j$ as $d_{ij}$.
--

- Want $\delta_{ij}$ to be similar to $d_{ij}$.


---

# Strain

- Assume Euclidean distances 
--
for now!
--

- Objective is to minimise strain defined as:

$$\sum\limits_{i=1}^{n-1}\sum\limits_{j=i+1}^n (\delta_{ij}^2-d_{ij}^2)$$
--

- This is known as classical MDS
--

- How can we solve this?

---

# Solution

1. Construct an $n\times n$ matrix of squared interpoint distances $\bDelta^{(2)}=\left\{\delta^2_{ij}\right\}$
--

2. Double center $\bDelta^{(2)}$ by computing $\bB=\bH'\bDelta^{(2)}\bH$ where $\bH=\bI-(1/n)\biota\biota'$
--

3. Find the eigenvalue decomposition of $\bB=\bU\bLambda\bU'$
--

4. The output coordinates are given by the first $m$ columns of $\bU\bLambda^{1/2}$

---

# Implementation

```{r,message=F,echo=T,warning=F}
library(tidyverse)
wb<-read_csv('../data/WorldBankClean.csv')
wb%>%
  select_if(.,is.numeric)%>% #Use numeric data
  scale()%>% #Standardise
  dist()%>% #Compute distance matrix
  cmdscale()%>% #MDS
  as_tibble(.name_repair = 'universal')%>%
  cbind(wb)->wb_mds

```

---

# Plot

```{r,echo=F,message=F,warning=F}
library(plotly)
library(widgetframe)
wb_mds%>%
  ggplot(aes(x=`...1`,y=`...2`,label=`Country Code`,text=`Country Name`))+geom_point(size=0.2)+
  geom_text(size=3,nudge_x=0.1)+xlab('Coordinate 1')+ylab('Coordinate 2')+coord_fixed()->g1
ggplotly(g1,dynamicTicks = T,tooltip="text")%>%
  frameWidget(width="100%",height="100%")  
  
```

---

# Look familiar??

- This is almost identical to Principal Components Analysis
--

- The axes have been flipped!
--

- PCA is invariant to reflections.
--
Why?
--

- MDS is also invariant to rotations.
--
Why?

---

# Why are they the same?

- Proof is a bit involved.
--

- The key idea to show that $\bB$ is related to $\bX\bX'$ while $\bS$ is related to $\bX'\bX$
--

- $\bX'\bX$ and $\bX\bX'$ have the same non-zero eigenvalues.
--

- Geometrically, the result makes sense by thinking about the extreme case where the data lie on a $m$-dimensional plane.

---
class:inverse,middle,center

# Beyond Euclidean Distance

---

# Non-Euclidean Distance

- What if a non-Euclidean distance is used?
--

- In this case classical MDS does not minimise Strain as defined previously, but minimises $tr(\bB-\bB^{*})$.
--

- Here $\bB^{*}$ is the doubly centered squared (Euclidean) distance matrix in the output space.
--

- Distances between output points faithfully represent distances between input points.
--

- Only use eigenvectors correponding to non-negative eigenvalues


---

# Implementation (L1)

```{r,message=F,echo=T,warning=F}
library(tidyverse)
wb<-read_csv('../data/WorldBankClean.csv')
wb%>%
  select_if(.,is.numeric)%>% #Use numeric data
  scale()%>% #Standardise
  dist(method = 'manhattan')%>% #Compute distance matrix
  cmdscale()%>% #MDS
  as_tibble(.name_repair = 'universal')%>%
  cbind(wb)->wb_mds_L1

```

---

# Plot (L1)

```{r,echo=F,fig.align='center'}
wb_mds_L1%>%
  ggplot(aes(x=`...1`,y=-`...2`,label=`Country Code`,text=`Country Name`))+geom_point(size=0.2)+
  geom_text(size=3,nudge_x = 0.1)+xlab('Coordinate 1')+ylab('Coordinate 2')+coord_fixed()
  
```

---


# Plot (L2)

```{r,echo=F,fig.align='center'}
wb_mds%>%
  ggplot(aes(x=`...1`,y=`...2`,label=`Country Code`,text=`Country Name`))+geom_point(size=0.2)+
  geom_text(size=3,nudge_x = 0.1)+xlab('Coordinate 1')+ylab('Coordinate 2')+coord_fixed()
  
```

---

# Why is this useful?

- We can have distances/dissimilarities between all sorts of objects
--
  
  + Time series
  + Functions
  + Probability distributions
  + Strings/ Texts

---

# A toy example

- Consider the word for "mother" in different languages
--

- The Levenshtein distance can be computed between strings
  + Counts number of insertions, deletions and substitutions to convert one string to another
--

- Pairwise Levenshtein distances computed and then classical multidimensional scaling applied.

---

#Languages

```{r, echo=FALSE,message=F,warning=F}
mother<-read_csv('../data/mother.csv')
n<-nrow(mother)
dd<-matrix(0,n,n)
for (i in 1:n){
  for (j in 1:n){
    dd[i,j]<-adist(mother$Word[i],mother$Word[j])
    dd[j,i]<-dd[i,j]
  }
}

df<-cbind(cmdscale(as.dist(dd)),mother)
colnames(df)[1:2]<-c("Comp. 1", "Comp. 2")

ggplot(df,aes(x=`Comp. 1`,y=`Comp. 2`,label=Word,text=Language))+geom_text()->g1

ggplotly(g1,tooltip = 'text')%>%frameWidget()


```

---

# Distance between pdfs

- Consider the electricity smart meter data.
--

- The distance between pdfs can be measured using a Jensen Shannon distance.
--

- This is the square root of the average of the Kullback Leibler divergence from $P$ to $Q$ and from $Q$ to $P$.
--

- For a log normal distribution this is available in closed form.
--

- Consider one household so that each observation corresponds to a time of week.

---


# Distance between pdfs

```{r,message=FALSE,echo=FALSE}

days<-c('Mon','Tue','Wed','Thu','Fri','Sat','Sun')
tod1<-paste0(str_pad(0:23,2,'left',pad='0'),':00')
tod2<-paste0(str_pad(0:23,2,'left',pad='0'),':30')
tod<-as.vector(rbind(tod1,tod2))
TimeOfWeek<-outer(days,tod,FUN=paste)%>%t()%>%as.vector()


sm<-read_csv('../data/SmartMeter.csv')
sm%>%filter(id==1321)%>%
  group_by(tow)%>%
  mutate(logd=log(demand))%>%
  summarise(mu=mean(logd),sigma2=var(logd))%>%
  mutate(TimeOfDay=ordered(tow%%48,labels=tod))->pars
n<-nrow(pars)
klds<-matrix(0,n,n)
for (i in 1:n){
  for (j in 1:n){
    if(i!=j){
      mui<-pars$mu[i]
      muj<-pars$mu[j]
      sigma2i<-pars$sigma2[i]
      sigma2j<-pars$sigma2[j]
      klds[i,j]<-log(sigma2j)-log(sigma2i)+
        ((sigma2i+(mui-muj)^2)/(2*sigma2j))-0.5
    }
  }
}
jsds<-sqrt((klds+t(klds))/2)

df<-cbind(cmdscale(as.dist(jsds)),pars)
colnames(df)[1:2]<-c("Comp. 1", "Comp. 2")


df%>%add_column(TimeOfWeek=TimeOfWeek)->df


ggplot(df,aes(x=`Comp. 1`,y=`Comp. 2`,col=TimeOfDay,label=TimeOfWeek))+geom_point()->g1
ggplotly(g1,tooltip = 'label')%>%frameWidget()


```

---
class: inverse, middle, center

# MDS: Beyond Linearity

---

# Beyond Classical MDS

- Classical MDS is designed to minimise Strain.
--

- An alternative  objective function called Stress can be minimised instead
--


$$\mbox{Stress}=\sum\limits_{i=1}^{n-1}\sum\limits_{j>i}\frac{(\delta_{ij}-d_{ij})^2}{\delta_{ij}}$$
--

- The difference between $\delta_{ij}$ and $d_{ij}$ acts like an error.
--

- The $\delta_{ij}$ on the denominator acts as a weight

---

#Weighting

- For large $\delta$ observations are far away in the input space.
--

  - For these pairs errors are more easily tolerated.
--

- For small $\delta$ observations are close in the input space.
--

  - For these pairs errors are not tolerated.
--

- The most accuracy is achieved for nearby points
--

- The local structure is preserved.

---

# Sammon mapping

- The Sammon mapping is solved by numerical optimisation (gradient descent).
--

- It is different from the classical solution
--

  - It is not based on an eigenvalue decomposition
--

  - It is not based on rotation
--

  - It is a non-linear mapping.

---

# Example

- The following is a simulated toy example to motivate non-linear dimension reduction.
--

- Consider the case where input points are 2D and the output points are 1D.
--

- The specific problem of doing multidimensional scaling where the lower dimension is 1 is called *seriation*.

---

# Original Data

```{r,echo=FALSE,message=FALSE,warning=FALSE,error=FALSE}
library(MASS)
set.seed(1)
xg<-seq(0,1,0.005)
signal<-1.2*sin((2*pi*xg)/2)
yg<-signal+0.01*rnorm(length(xg))
raw<-cbind(xg,yg)
theta<-pi/4
rotm<-matrix(c(cos(theta),-sin(theta),sin(theta),cos(theta)),2,2)
rot<-raw%*%rotm
dd<-dist(rot)
pca<-prcomp(rot)
cmds<-cmdscale(dd,k = 1)
smds<-sammon(dd,k=1,trace = FALSE)
smds<-sammon(dd,y = matrix(-xg,ncol = 1),k=1,trace = FALSE)
df<-tibble(rank=xg,
           x=-rot[,1],
           y=rot[,2],
           classic=cmds,
           sammon=smds$points,
           xrot=pca$x[,1],
           yrot=pca$x[,2])
ggplot(df,aes(x=x,y=y))+geom_point()+coord_fixed()
```

---

# Original Data

```{r,echo=FALSE}
ggplot(df,aes(x=x,y=y,col=rank))+geom_point()+scale_color_viridis_c()+coord_fixed()
```

---

# Rotate (Classical Solution)

```{r,echo=FALSE}
ggplot(df,aes(x=xrot,y=yrot,col=rank))+geom_point()+scale_color_viridis_c()+coord_fixed()
```

---

# Keep 1 Dimension

```{r,echo=FALSE}
ggplot(df,aes(x=classic,y=0,col=rank))+geom_point()+scale_color_viridis_c()
```



---

# Sammon Mapping

```{r,echo=FALSE}
ggplot(df,aes(x=sammon,y=0,col=rank))+geom_point()+scale_color_viridis_c()
```


---

# Discussion

- Classical MDS cannot account for non-linearity.
  + The dark blue and yellow points are represented as close to one another.
--

- Sammon does account for non-linearity.
  + The blue and yellow points are represented as far apart.
--

- The Sammon mapping preserves local structure.

---

# Summary

- Classical MDS with Euclidean input distances is identical (up to rotation) to PCA
--

- This no longer holds with non-Euclidean input distances
--

  + Classical MDS gives output points that 'approximate' information in the double centered squared distance matrix.
--

- The Sammon mapping highlights the importance of preserving local structure.
  

---

class: inverse, center, middle

#Questions?
